# CMPT 756 Project Performance Test Result Analyzer

This repository contains tools to visualize and compare performance test results across multiple deployment environments for CMPT756 Project. It reads summary and time-series data from CSV files (e.g., generated by Locust or similar tools) and produces bar charts and time-based line charts.

## ğŸ” Features

- Compare **Median Response Time**, **Average Response Time**, and **Requests per Second** across environments.
- Plot **Requests/s**, **Failures/s**, and **Average Response Time** over time (normalized so each test starts at 0s).
- Supports multiple environments, such as:
  - baseline
  - replica
  - CPU/memory constrained
  - HPA (Horizontal Pod Autoscaler)
  - Istio-enabled

## ğŸ“ File Structure

.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ results_stats.csv          # Summary stats per environment
â”‚   â””â”€â”€ results_stats_history.csv  # Time-series stats per environment
â”œâ”€â”€ run_local_test.py                  # Main analysis 
â”œâ”€â”€ vis.py               				# plotting script
â”œâ”€â”€ *.png                               # Output plots will be saved here
â””â”€â”€ README.md

## ğŸ“¦ Requirements

- Python 3.7+
- pandas
- matplotlib

Install dependencies (if needed):

```bash
pip install pandas matplotlib

ğŸš€ Usage

Place all results_*.csv files in the data/ directory and run:

python analyze_results.py

The script will:
	1.	Parse and compare key metrics (Median Response Time, Average Response Time, Requests/s) across environments.
	2.	Normalize time and generate over-time plots for:
	â€¢	Requests per second
	â€¢	Failures per second
	â€¢	Average response time

ğŸ“Š Output

The following images will be saved in the current directory:
	â€¢	median_response_time_comparison.png
	â€¢	average_response_time_comparison.png
	â€¢	requests_per_second_comparison.png
	â€¢	requests_over_time.png
	â€¢	failures_over_time.png
	â€¢	avg_resp_time_over_time.png


