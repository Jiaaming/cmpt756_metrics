# CMPT 756 Project Performance Test Result Analyzer

This repository contains tools to visualize and compare performance test results across multiple deployment environments for CMPT756 Project. It reads summary and time-series data from CSV files (e.g., generated by Locust or similar tools) and produces bar charts and time-based line charts.

## 🔍 Features

- Compare **Median Response Time**, **Average Response Time**, and **Requests per Second** across environments.
- Plot **Requests/s**, **Failures/s**, and **Average Response Time** over time (normalized so each test starts at 0s).
- Supports multiple environments, such as:
  - baseline
  - replica
  - CPU/memory constrained
  - HPA (Horizontal Pod Autoscaler)
  - Istio-enabled

## 📁 File Structure

.
├── data/
│   ├── results_stats.csv          # Summary stats per environment
│   └── results_stats_history.csv  # Time-series stats per environment
├── run_local_test.py                  # Main analysis 
├── vis.py               				# plotting script
├── *.png                               # Output plots will be saved here
└── README.md

## 📦 Requirements

- Python 3.7+
- pandas
- matplotlib

Install dependencies (if needed):

```bash
pip install pandas matplotlib

🚀 Usage

Place all results_*.csv files in the data/ directory and run:

python analyze_results.py

The script will:
	1.	Parse and compare key metrics (Median Response Time, Average Response Time, Requests/s) across environments.
	2.	Normalize time and generate over-time plots for:
	•	Requests per second
	•	Failures per second
	•	Average response time

📊 Output

The following images will be saved in the current directory:
	•	median_response_time_comparison.png
	•	average_response_time_comparison.png
	•	requests_per_second_comparison.png
	•	requests_over_time.png
	•	failures_over_time.png
	•	avg_resp_time_over_time.png


