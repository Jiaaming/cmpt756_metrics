# CMPT 756 Project Performance Test Result Analyzer

This repository contains tools to visualize and compare performance test results across multiple deployment environments for **CMPT756** Project. It reads summary and time-series data from CSV files (e.g., generated by Locust or similar tools) and produces bar charts and time-based line charts.

## 🔍 Features

- Compare **Median Response Time**, **Average Response Time**, and **Requests per Second** across environments.
- Plot **Requests/s**, **Failures/s**, and **Average Response Time** over time (normalized so each test starts at 0s).
- Supports multiple environments, such as:
  - baseline
  - replica
  - CPU/memory constrained
  - HPA (Horizontal Pod Autoscaler)
  - Istio-enabled


## 📦 Requirements

- Python 3.7+
- pandas
- matplotlib



## 🚀 Usage

Place all results_*.csv files in the data/ directory and run:
```
python run_local_test.py # Main test script

python vis.py
```
These scripts will:
	1.	Parse and compare key metrics (Median Response Time, Average Response Time, Requests/s) across environments.
	2.	Normalize time and generate over-time plots for:
	•	Requests per second
	•	Failures per second
	•	Average response time

## 📊 Output

The following images will be saved in the current directory:
- median_response_time_comparison.png
- average_response_time_comparison.png
- requests_per_second_comparison.png
- requests_over_time.png
- failures_over_time.png
- avg_resp_time_over_time.png


