# CMPT 756 Project Performance Test Result Analyzer

This repository contains tools to visualize and compare performance test results across multiple deployment environments for **CMPT756** Project. It reads summary and time-series data from CSV files (e.g., generated by Locust or similar tools) and produces bar charts and time-based line charts.

## ğŸ” Features

- Compare **Median Response Time**, **Average Response Time**, and **Requests per Second** across environments.
- Plot **Requests/s**, **Failures/s**, and **Average Response Time** over time (normalized so each test starts at 0s).
- Supports multiple environments, such as:
  - baseline
  - replica
  - CPU/memory constrained
  - HPA (Horizontal Pod Autoscaler)
  - Istio-enabled


## ğŸ“¦ Requirements

- Python 3.7+
- pandas
- matplotlib



## ğŸš€ Usage

Place all results_*.csv files in the data/ directory and run:
```
python run_local_test.py # Main test script

python vis.py
```
These scripts will:
	1.	Parse and compare key metrics (Median Response Time, Average Response Time, Requests/s) across environments.
	2.	Normalize time and generate over-time plots for:
	â€¢	Requests per second
	â€¢	Failures per second
	â€¢	Average response time

## ğŸ“Š Output

The following images will be saved in the current directory:
- median_response_time_comparison.png
- average_response_time_comparison.png
- requests_per_second_comparison.png
- requests_over_time.png
- failures_over_time.png
- avg_resp_time_over_time.png


